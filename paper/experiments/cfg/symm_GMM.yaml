defaults:
  - override hydra/launcher: joblib  # Parallel runs

seed: -1
debug: False
exp_name: "NCP-GMM-C2"
exp_label: "test"
device: 0  # -1 for all GPUs [0, ..., gpu_id] for a specific GPU

# GMM Symmetry properties
symm_group: C2 # CN / DN / Ico / Octa   # Hypothesis symmetry group ASSUMED for the GMM dataset and NNs.
#x_symm_subgroup_id: "trivial"                # ACTUAL/EMPIRICAL symmetry subgroup used for the GMM
#y_symm_subgroup_id: "trivial"                # ACTUAL/EMPIRICAL symmetry subgroup used for the GMM
x_symm_subgroup_id: null                 # ACTUAL/EMPIRICAL symmetry subgroup used for the GMM
y_symm_subgroup_id: null                 # ACTUAL/EMPIRICAL symmetry subgroup used for the GMM
regular_multiplicity: 0                 # Multiplicity of the regular representation

# MODEL CONFIG
model: NCP
truncated_op_bias: 'full_rank'  # 'Cxy' or 'diag' or 'svals'
gamma: 0.5               # Weight of the NCP regularization loss term. NCP train best at low gamma values < 0.01
constraint_out_irreps_dim: False

embedding:
  hidden_units: 32
  hidden_layers: 3
  embedding_dim: 10
  activation: ELU

# OPTIMIZATION CONFIG
lr: 5e-4           # Learning rate NCP train best at low lr values < 1e-3
patience: 5       # Epochs to wait for improvement before early stopping
batch_size: 1024   # The higher the best
max_epochs: 500   # Number of passes over ${gmm.n_samples} before stop training. Early stopping is used.
data_on_device: False  # Load dataset to GPU (usefull for fast training of small datasets)
train_samples_ratio: 0.7    # Between 0.7 and 0.1 of n_total_samples samples

# EXPERIMENT CONFIG
gmm:
  n_kernels: 5
  n_total_samples: 20000
  means_max_norm: 3.0    # Standard deviation used to choose the centers of the Gaussians from a normal N dimensional dist.
  seed: 0
  # Seed for the reproducibility of the GMM dataset


embedding_desc: "hu${embedding.hidden_units}_hl${embedding.hidden_layers}_ed${embedding.embedding_dim}_act${embedding.activation}"
gmm_desc: "G=${symm_group} Hx=${x_symm_subgroup_id} Hy=${y_symm_subgroup_id} train_ratio=${train_samples_ratio}/nk${gmm.n_kernels}_ns${gmm.n_total_samples}_dim=${regular_multiplicity}-gmm_seed=${gmm.seed}"
model_desc: "${model}_gamma${gamma}_lr${lr}_bs${batch_size}"

hydra:
  run:
    dir: "../experiment_results/${exp_name}/${exp_label}/${gmm_desc}/${model_desc}--${embedding_desc}/${hydra.job.override_dirname} seed=${seed}"

  sweep:
    dir: ./experiments/${exp_name}/${exp_label}/${gmm_desc}/
    subdir: ${model_desc}--${embedding_desc}/${hydra.job.override_dirname} seed=${seed}

  job:
    name: ${exp_name}
    num: ${seed}
    env_set:
      HYDRA_FULL_ERROR: '1'
      WANDB_START_METHOD: 'thread'
    config:
      override_dirname:
        kv_sep: "="
        item_sep: " "
        exclude_keys:
          - debug
          - device
          - seed
          - exp_name
          - exp_label
          - max_epochs
          - embedding.hidden_units
          - embedding.hidden_layers
          - embedding.embedding_dim
          - embedding.activation
          - gmm.n_kernels
          - model
          - gamma
          - lr
          - batch_size
          - max_epochs
          - symm_group
          - x_symm_subgroup_id
          - y_symm_subgroup_id
